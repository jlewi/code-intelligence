{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "* Some experiments building a chatbot using Dialogflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding /home/jovyan/git_kubeflow-code-intelligence/py to python path\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "import sys\n",
    "import notebook_setup\n",
    "\n",
    "reload(notebook_setup)\n",
    "notebook_setup.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create documents\n",
    "\n",
    "* We want to create documents for a [Dialogflow knowledgebase](https://cloud.google.com/dialogflow/docs/how/knowledge-bases#supported-content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_intelligence import github_util\n",
    "from code_intelligence import graphql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GraphQLClient is defaulting to FixedAccessTokenGenerator based on environment variables. This is deprecated. Caller should explicitly pass in a instance via header_generator. Traceback:\n",
      "<function extract_stack at 0x7f38088946a8>\n"
     ]
    }
   ],
   "source": [
    "gh_client = graphql.GraphQLClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_url = \"https://github.com/kubeflow/kubeflow/issues/4259\"\n",
    "issue_data = github_util.get_issue(issue_url, gh_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GCP] Node Auto provisioner pool is missing VM service account and oauth scopes\n",
      "/kind bug\n",
      "\n",
      "**What steps did you take and what happened:**\n",
      "After deploying kubeflow successfully on GCP I'm trying to run a pipeline.\n",
      "Components in the pipelines tries to pull a private container from gcr.io fails to authenticate. See description below.\n",
      "\n",
      "The following code is used to define the pipeline component:\n",
      "\n",
      "```\n",
      "    train = dsl.ContainerOp(\n",
      "        name='train',\n",
      "        image='gcr.io/<project>/<container>:latest',\n",
      "        arguments=[ .... ]\n",
      "    ).set_gpu_limit(1)\n",
      "\n",
      " steps = [train]\n",
      "    for step in steps:\n",
      "        step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
      "```\n",
      "\n",
      "Now the relevant parts of the compiled yaml file for the pipeline:\n",
      "```\n",
      "\n",
      "  - container:\n",
      "      env:\n",
      "      - name: GOOGLE_APPLICATION_CREDENTIALS\n",
      "        value: /secret/gcp-credentials/user-gcp-sa.json\n",
      "      - name: CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE\n",
      "        value: /secret/gcp-credentials/user-gcp-sa.json\n",
      "      image: gcr.io/<project>/<container>:latest\n",
      "      resources:\n",
      "        limits:\n",
      "          nvidia.com/gpu: 1\n",
      "      volumeMounts:\n",
      "      - mountPath: /secret/gcp-credentials\n",
      "        name: gcp-credentials-user-gcp-sa\n",
      "    inputs:\n",
      "    name: train\n",
      "    volumes:\n",
      "    - name: gcp-credentials-user-gcp-sa\n",
      "      secret:\n",
      "        secretName: user-gcp-sa\n",
      "```\n",
      "After initialization the pod falls over with the following:\n",
      "\n",
      ">   Normal   Pulling           56s                    kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  pulling image \"argoproj/argoexec:v2.3.0\"\n",
      ">   Normal   Pulled            48s                    kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Successfully pulled image \"argoproj/argoexec:v2.3.0\"\n",
      ">   Normal   Started           44s                    kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Started container\n",
      ">   Normal   Created           44s                    kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Created container\n",
      ">   Normal   BackOff           16s (x2 over 43s)      kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Back-off pulling image \"gcr.io/project/container:latest\"\n",
      ">   Warning  Failed            16s (x2 over 43s)      kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Error: ImagePullBackOff\n",
      ">   Normal   Pulling           2s (x3 over 44s)       kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  pulling image \"gcr.io/project/container:latest\"\n",
      ">   Warning  Failed            2s (x3 over 44s)       kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Failed to pull image \"gcr.io/project/container:latest\": rpc error: code = Unknown desc = Error response from daemon: unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication\n",
      ">   Warning  Failed            2s (x3 over 44s)       kubelet, gke-pipelines-nap-n1-standard-1-gpu1--30d96aa7-5sq4  Error: ErrImagePull\n",
      "> \n",
      "\n",
      "\n",
      "**Anything else you would like to add:**\n",
      "\n",
      "I have noticed that the user-gcp-sa.json file is not in /secret but rather in /mainctrfs, could this be causing problems?\n",
      "\n",
      "Entering the relevant pod with a shell shows:\n",
      "```\n",
      "root@pod-b9jks-3315453858:/# find / -name \"user-gcp-sa.json\"\n",
      "/mainctrfs/secret/gcp-credentials/user-gcp-sa.json\n",
      "/mainctrfs/secret/gcp-credentials/..2019_10_09_10_01_29.770571990/user-gcp-sa.json\n",
      "```\n",
      "\n",
      "All I could find in this was https://www.kubeflow.org/docs/gke/authentication/#authentication-from-kubeflow-pipelines which does not tell you much other than to do what I'm describing.\n",
      "\n",
      "**Environment:**\n",
      "\n",
      "- Kubeflow version: build commit 812ca7f\n",
      "- kfctl version: kfctl v0.6.2-0-g47a0e4c7\n",
      "- Kubernetes platform: gcp 1.12.10-gke.5\n",
      "- kubectl version: 1.6\n",
      "- OS: Ubuntu Bionic x64\n",
      "\n",
      "Python module version:\n",
      "kfp (0.1.31.2)\n",
      "kfp-server-api (0.1.18.3)\n",
      "\n",
      "Issue Label Bot is not confident enough to auto-label this issue.\n",
      "            See [dashboard](https://mlbot.net/data/kubeflow/kubeflow) for more details.\n",
      "            \n",
      "@Toeplitz How did you deploy Kubeflow? GKE uses the VM service account by default to pull GCR images. So you likely don't have a VM service account with proper IAM roles or you haven't set the OAuth scopes on the VM.\n",
      "\n",
      "Alternatively you can set image pull secrets\n",
      "https://medium.com/@michaelmorrissey/using-cross-project-gcr-images-in-gke-1ddc36de3d42\n",
      "\n",
      "\n",
      "@jlewi I deployed using the CLI instructions https://www.kubeflow.org/docs/gke/deploy/deploy-cli/ with Oauth and I'm accessing kubeflow through https://KFAPP.endpoints.project-id.cloud.goog/\n",
      "\n",
      "The service accounts are created:\n",
      "kfapp-user@project.iam.gserviceaccount.com (has Storage Admin and project viewer), kfapp-vm@project.iam.gserviceaccount.com (has Storage Object Viewer) so  that looks correct to me in that both of those should have access to GCR pulls? \n",
      "\n",
      "I'm not sure how to check for OAuth scopes on the VM ?\n",
      "@Toeplitz I suspect you are running into an issue with the node auto-provisioner not setting the service account and scopes correctly.\n",
      "\n",
      "Can you provide the output of \n",
      "\n",
      "```\n",
      "kubectl describe pods -o yaml ${POD}\n",
      "```\n",
      "\n",
      "That should provide the name of the node its running on\n",
      "\n",
      "\n",
      "Then do\n",
      "\n",
      "```\n",
      "gcloud --project=${PROJECT} compute instances describe --zone=${ZONE} $INSTANCE\n",
      "```\n",
      "\n",
      "Assuming you are running into problems with the node autoprovisioner here's how to fix this\n",
      "\n",
      "Run the commands below to set the service account and oauth scopes for the auto-provisioner\n",
      "\n",
      "```\n",
      "gcloud --project=${PROJECT} beta container clusters update ${KFAPP} --enable-autoprovisioning --autoprovisioning-service-account=${KFAPP}-vm@${PROJECT}.iam.gserviceaccount.com --zone=${ZONE}\n",
      "\n",
      "gcloud --project=${PROJECT} beta container clusters update ${KFAPP} --enable-autoprovisioning --autoprovisioning-scopes=https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/devstorage.read_only\n",
      "```\n",
      "\n",
      "Then look for an autoprovisioned node pool (it should have `nap` in the name`)\n",
      "\n",
      "```\n",
      "gcloud --project=${PROJECT} container node-pools list --zone=${ZONE} --cluster=${CLUSTER}\n",
      "```\n",
      "\n",
      "Then delete that node pool\n",
      "\n",
      "\n",
      "```\n",
      "gcloud --project=${PROJECT} container node-pools delete --zone=${ZONE} --cluster=${CLUSTER} ${NODEPOOL}\n",
      "```\n",
      "\n",
      "The next time a new node pool is created it will have the newly added scope and VM service account.\n",
      "\n",
      "\n",
      "\n",
      "@jlewi Thank you for the answer. It turns out that the accounts where not setup correctly on the autoprovisioned gpu nodes. Your suggestion fixed the issue.\n",
      "\n",
      "Am I wrong to think this should be added to the default setup?\n",
      "\n",
      "As a small aside, is it possible to have the autoprovisioned node become n1-standard-8 instead of n1-standard-1 ?\n",
      "@Toeplitz yes we should be configuring the default service account on the auto-provisioned pool.\n",
      "\n",
      "I don't think we set the default settings for the NAP pool; the whole idea of NAP is that it picks a VM size based on the resources requested by your pod.\n",
      "\n",
      "/cc @amygdala \n",
      "This should be fixed kubeflow/manifests#498. We just need to wait for the next 0.7 RC and then verify it is fixed before closing this bug.\n",
      "Using kubeflow/manifests 99246ddd4b2503b57b44a908ff0a7e41c4de4b0a\n",
      "\n",
      "I used GPUs to force it onto an nap node. Unfortunately it still can't pull the image there is an authorization issue\n",
      "\n",
      "[pod.describe.txt](https://github.com/kubeflow/kubeflow/files/3721353/pod.describe.txt)\n",
      "\n",
      "Here's the node spec\n",
      "[nap-vm.txt](https://github.com/kubeflow/kubeflow/files/3721355/nap-vm.txt)\n",
      "\n",
      "```\n",
      "serviceAccounts:\n",
      "- email: kftest-1012-170754-vm@jlewi-dev.iam.gserviceaccount.com\n",
      "  scopes:\n",
      "  - https://www.googleapis.com/auth/logging.write\n",
      "  - https://www.googleapis.com/auth/monitoring\n",
      "```\n",
      "\n",
      "So its using the right service account but is missing oauth scope for storage.\n",
      "Here's the iam policy \n",
      "[iam-policy.txt](https://github.com/kubeflow/kubeflow/files/3721359/iam-policy.txt)\n",
      "\n",
      "The service account has objectViewer permission.\n",
      "\n",
      "\n",
      "So I confirmed with node autoprovisioning team and there is a known issue.\n",
      "\n",
      "The next release of GKE will support setting both service account and scopes. \n",
      "\n",
      "So here's where things stand\n",
      "\n",
      "* In 0.7.0 we will set the service account \n",
      "* In 0.7.0 users will still need to run the gcloud command to set scopes on NAP\n",
      "\n",
      "  ```\n",
      "  gcloud --project=${PROJECT} beta container clusters update ${KFAPP} --enable-autoprovisioning --autoprovisioning-scopes=https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/devstorage.read_only\n",
      "  ```\n",
      "\n",
      "* In 0.7.X once GKE has rolled out the fix to allow setting both we will update our DM configs to set both.\n",
      "\n",
      "* Downgrading to P1 since this is not going to block 0.7.0 and we are waiting on a GKE push\n",
      "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.\n",
      "\n",
      "/llifecycle frozen\n",
      "\n",
      "The API changes to allow setting both scopes and service account should be rolled out. It should be easy to update the deployment manager configs and then verify this is working.\n",
      "\n",
      "We should do this for 1.0.\n",
      "Fix is cherry-picked onto both release and master branches.\n",
      "\n",
      "The full spec for a 1.0 GKE cluster is below. Looking at the node autoprovisioning settings both service accounts and oauth scopes are set.\n",
      "\n",
      "```\n",
      "autoscaling:\n",
      "  autoprovisioningNodePoolDefaults:\n",
      "    oauthScopes:\n",
      "    - https://www.googleapis.com/auth/logging.write\n",
      "    - https://www.googleapis.com/auth/monitoring\n",
      "    - https://www.googleapis.com/auth/devstorage.read_only\n",
      "    serviceAccount: kf-v1-01300120-b52-vm@kubeflow-ci-deployment.iam.gserviceaccount.com\n",
      "  enableNodeAutoprovisioning: true\n",
      "  resourceLimits:\n",
      "  - maximum: '128'\n",
      "    resourceType: cpu\n",
      "  - maximum: '2000'\n",
      "    resourceType: memory\n",
      "  - maximum: '16'\n",
      "    resourceType: nvidia-tesla-k80\n",
      "```\n",
      "\n",
      "[kf-v1-01300120-b52.yaml.txt](https://github.com/kubeflow/kubeflow/files/4135208/kf-v1-01300120-b52.yaml.txt)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{issue_data['title']}\")\n",
    "body = \"\\n\".join(issue_data[\"comments\"])\n",
    "print(f\"{body}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
