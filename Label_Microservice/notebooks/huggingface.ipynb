{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "* Train a model using Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.cloud import storage\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from code_intelligence import gcs_util\n",
    "import io\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import datettime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import notebook_setup\n",
    "\n",
    "notebook_setup.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Python Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"py\"))\n",
    "\n",
    "if not py_dir in sys.path:\n",
    "    logging.info(f\"Adding {py_dir} to path\")\n",
    "    sys.path.append(py_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Issues:\n",
    "#     def __init__(self):\n",
    "#         # The path to the GCS file containing the text\n",
    "#         self.text_file = \"\"\n",
    "#         # List of labels to apply to this file.\n",
    "#         self.labels = []\n",
    "        \n",
    "class GCSDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Implement a dataset class which reads the files from GCS.\"\"\"\n",
    "    def __init__(self, input_files, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV file containing a list of files and the labels\n",
    "# This file is produced by the notebook automl.ipynb\n",
    "files_list = \"gs://issue-label-bot-dev_automl/automl_TCN8830229559715561472/dataset_201103_151334.csv\"\n",
    "\n",
    "bucket_name, obj_path = gcs_util.split_gcs_uri(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = storage_client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = b.blob(obj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_contents = o.download_as_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.read_csv(io.BytesIO(csv_contents), names=[\"garbage\", \"file\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[\"text\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the actual file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of items related to progress\n",
    "percent_interval = 1\n",
    "progress = np.floor(files.shape[0] * percent_interval / 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent done 0.0; Processing 1 of 9890\n"
     ]
    }
   ],
   "source": [
    "# TODO(jlewi): Reading from GCS may not be efficient. The GCS data is obtained in automl.ipynb by creating a dataframe from BigQuery\n",
    "# It might be faster just to read the data directly from BigQuery and if necessary store the dataframe as an hdf5 file\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "for index in range(files.shape[0]):\n",
    "    if index % progress == 0:        \n",
    "        percent = index/files.shape[0] * 100\n",
    "        elapsed = datetime.datetime.now() - start\n",
    "        print(f\"Percent done {percent}; Processing {index+1} of {files.shape[0]}; elapsed {elapsed}\")\n",
    "    bucket_name, obj_path = gcs_util.split_gcs_uri(files_list)\n",
    "    b = storage_client.bucket(bucket_name)\n",
    "    o = b.blob(obj_path)\n",
    "    files.at[index, \"text\"] = o.download_as_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(x):\n",
    "    if isinstance(x, float):\n",
    "        return []\n",
    "    \n",
    "    return [i.strip() for i in x.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[\"labels\"] = files[\"labels_raw\"].apply(split_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "train_files, val_files = model_selection.train_test_split(files, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique list of labels \n",
    "target_labels = functools.reduce(np.union1d, files[\"labels\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model \n",
    "\n",
    "* Use [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/classification) which is built ontop of HuggingFace\n",
    "* You can check out this [example](https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py) which just uses HuggingFace\n",
    "* This code is based on the vscode [issue label model](https://github.com/microsoft/vscode-github-triage-actions/blob/master/classifier-deep/train/vm-filesystem/classifier/generateModels.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(jlewi): This was copied code from vscode. What do we want to do with the logging module?\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClassificationArgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-037b233ff745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"area\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model_args = ClassificationArgs(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_model_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_model_best\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClassificationArgs' is not defined"
     ]
    }
   ],
   "source": [
    "category = \"area\"\n",
    "\n",
    "model_args = ClassificationArgs(\n",
    "    output_dir=category + \"_model\",\n",
    "    best_model_dir=category + \"_model_best\",\n",
    "    overwrite_output_dir=True,\n",
    "    train_batch_size=16,\n",
    "    eval_batch_size=32,\n",
    "    max_seq_length=256,\n",
    "    num_train_epochs=2,\n",
    "    save_model_every_epoch=False,\n",
    "    save_eval_checkpoints=False,    \n",
    ")\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average=\"micro\")\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    # TODO(jlewi): bert finetuned isn't found.\n",
    "    # So I used what I found in https://github.com/ThilinaRajapakse/simpletransformers/blob/master/examples/text_classification/multilabel_classification.py\n",
    "    # \"bert\", \"finetuned\", \n",
    "    \"roberta\", \"roberta-base\",\n",
    "    num_labels=len(target_labels), args=model_args,\n",
    "    use_cuda=False,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(\n",
    "    train_df, eval_df=test_df, output_dir=category + \"_model/checkpoints\",    \n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(\n",
    "    test_df,\n",
    "    output_dir=category + \"_model/eval\",\n",
    "    f1=f1_multiclass,\n",
    "    acc=accuracy_score,\n",
    ")\n",
    "\n",
    "with open(os.path.join(category + \"_model\", \"target_names.json\"), \"w\") as f:\n",
    "    json.dump(target_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ec6206aa1935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
